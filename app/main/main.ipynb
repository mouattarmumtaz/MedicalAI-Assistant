{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3261dfe8",
   "metadata": {},
   "source": [
    "# This script builds a Medical AI Assistant that combines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa788e26",
   "metadata": {},
   "source": [
    "### Chest X-ray classification (Normal vs Pneumonia using ResNet18)\n",
    "\n",
    "### Medical report summarization (using HuggingFace Transformers)\n",
    "\n",
    "### Guideline-based Q&A with Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Interactive Gradio web app interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1017d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re, shutil, string, torch\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline as hf_pipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Paths \n",
    "ROOT = r\"D:\\\\MedicalAI-Assistant\"\n",
    "DATA_DIR = os.path.join(ROOT, 'data')\n",
    "ART_DIR = os.path.join(ROOT, 'artifacts')\n",
    "XWEIGHTS = os.path.join(ART_DIR, 'xray_model.pth')\n",
    "TCAL = os.path.join(ART_DIR, 'xray_temp.pt')\n",
    "SUM_DIR = os.path.join(ART_DIR, 'summarizer')\n",
    "PDF_PATH = os.path.join(DATA_DIR, 'guidelines.pdf')\n",
    "INDEX_DIR = os.path.join(ART_DIR, 'faiss_index')\n",
    "os.makedirs(ART_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLASS_NAMES = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "#  X-ray model \n",
    "from torchvision import models, transforms\n",
    "from torch import nn\n",
    "\n",
    "def load_resnet18(weights_path: str, num_classes=2):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    in_feats = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "    sd = torch.load(weights_path, map_location=DEVICE)\n",
    "    if isinstance(sd, dict) and 'state_dict' in sd:\n",
    "        model.load_state_dict(sd['state_dict'], strict=False)\n",
    "    else:\n",
    "        model.load_state_dict(sd, strict=False)\n",
    "    model.eval().to(DEVICE)\n",
    "    return model\n",
    "\n",
    "_to_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "clf = load_resnet18(XWEIGHTS)\n",
    "T = torch.load(TCAL, map_location='cpu').get('T', 1.0) if os.path.exists(TCAL) else 1.0\n",
    "\n",
    "def predict_xray(img):\n",
    "    if isinstance(img, (str, os.PathLike)):\n",
    "        img = Image.open(img).convert('RGB')\n",
    "    x1 = _to_resnet(img).unsqueeze(0)\n",
    "    x2 = _to_resnet(img.transpose(Image.FLIP_LEFT_RIGHT)).unsqueeze(0)\n",
    "    X = torch.cat([x1, x2], 0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = clf(X).mean(0, keepdim=True)\n",
    "        logits = logits / T\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        idx = int(probs.argmax())\n",
    "    return idx, float(probs[idx]), probs.tolist()\n",
    "\n",
    "UNCERTAIN_THRESH = 0.55\n",
    "\n",
    "def decision_text(pred_idx, conf):\n",
    "    label = CLASS_NAMES[pred_idx]\n",
    "    if conf < UNCERTAIN_THRESH:\n",
    "        return f\"Normal â€” No {label}. Doesn't need clinical correlation.\"\n",
    "    else:\n",
    "        return f\"Prediction: {label}\"\n",
    "\n",
    "\n",
    "NORMAL_REPORT = (\n",
    "    \"Chest X-ray (PA):\\n\"\n",
    "    \"â€¢ Cardiomediastinal silhouette: Within normal limits.\\n\"\n",
    "    \"â€¢ Lungs: No focal consolidation identified. No pleural effusion.\\n\"\n",
    "    \"â€¢ Pneumothorax: Not seen.\\n\"\n",
    "    \"â€¢ Bones/soft tissues: Unremarkable.\\n\\n\"\n",
    "    \"Impression: No acute cardiopulmonary abnormality.\"\n",
    ")\n",
    "\n",
    "# Summarizer \n",
    "if os.path.isdir(SUM_DIR):\n",
    "    sum_tok = AutoTokenizer.from_pretrained(SUM_DIR)\n",
    "    sum_mod = AutoModelForSeq2SeqLM.from_pretrained(SUM_DIR)\n",
    "else:\n",
    "    sum_tok = AutoTokenizer.from_pretrained('t5-small')\n",
    "    sum_mod = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "\n",
    "SUM = hf_pipeline('summarization', model=sum_mod, tokenizer=sum_tok)\n",
    "\n",
    "#  RAG \n",
    "EMB = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    t = re.sub(r'[^A-Za-z0-9.,;:%Â°()\\-â€“/\\[\\] ]+', ' ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t)\n",
    "    return t.strip()\n",
    "\n",
    "# Build FAISS if missing\n",
    "if not os.path.isdir(INDEX_DIR) and os.path.exists(PDF_PATH):\n",
    "    docs = PyPDFLoader(PDF_PATH).load()\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        c = clean_text(d.page_content)\n",
    "        for i in range(0, len(c), 800):\n",
    "            chunks.append(c[i:i+800])\n",
    "    vs = FAISS.from_documents([Document(page_content=x) for x in chunks], EMB)\n",
    "    vs.save_local(INDEX_DIR)\n",
    "\n",
    "RETRIEVER = FAISS.load_local(\n",
    "    INDEX_DIR, EMB, allow_dangerous_deserialization=True\n",
    ").as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "llm_tok = AutoTokenizer.from_pretrained('google/flan-t5-small')\n",
    "llm_mod = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-small')\n",
    "llm_pipe = hf_pipeline(\n",
    "    'text2text-generation',\n",
    "    model=llm_mod,\n",
    "    tokenizer=llm_tok,\n",
    "    max_new_tokens=160,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "LLM = HuggingFacePipeline(pipeline=llm_pipe)\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    template=(\n",
    "        \"You are a medical guideline assistant. Answer ONLY from the context.\\n\"\n",
    "        \"If the answer is not in the context, say: 'I don't know'.\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "QA = RetrievalQA.from_chain_type(\n",
    "    llm=LLM,\n",
    "    retriever=RETRIEVER,\n",
    "    chain_type='stuff',\n",
    "    chain_type_kwargs={'prompt': PROMPT},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "#  Gibberish guard \n",
    "def looks_like_gibberish(s: str) -> bool:\n",
    "    if not s or len(s) < 8:\n",
    "        return True\n",
    "    letters = sum(c.isalpha() for c in s)\n",
    "    printable = sum(c in string.printable for c in s)\n",
    "    return (letters / len(s) < 0.4) or (printable / len(s) < 0.9)\n",
    "\n",
    "#  Gradio UI \n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('# ðŸ©º Medical AI Assistant â€” X-ray + Reports + RAG')\n",
    "    with gr.Row():\n",
    "        xray = gr.Image(label='Upload Chest X-ray', type='pil')\n",
    "        report = gr.Textbox(lines=12, label='Paste medical report / notes')\n",
    "        question = gr.Textbox(label='Ask a medical guideline question')\n",
    "        btn = gr.Button('Analyze')\n",
    "\n",
    "    xray_out = gr.Textbox(label='X-ray result')\n",
    "    summary_out = gr.Textbox(label='Report summary')\n",
    "    answer_out = gr.Textbox(label='Guideline answer (RAG)')\n",
    "\n",
    "    # define pipeline function\n",
    "    def run_pipeline(xray_image, report_text, question):\n",
    "        # X-ray prediction\n",
    "        pred_idx, prob, _ = predict_xray(xray_image)\n",
    "        xray_res = decision_text(pred_idx, prob)\n",
    "\n",
    "        # Report summarization\n",
    "        summary = ''\n",
    "        if report_text and report_text.strip():\n",
    "            try:\n",
    "                summary = SUM(report_text, max_length=120, min_length=20, do_sample=False)[0]['summary_text']\n",
    "            except Exception as e:\n",
    "                summary = f'Summarization error: {e}'\n",
    "\n",
    "        # RAG answer\n",
    "        answer = ''\n",
    "        if question and question.strip():\n",
    "            try:\n",
    "                res = QA.invoke({'query': question})\n",
    "                answer = res['result'] if isinstance(res, dict) else str(res)\n",
    "\n",
    "                if looks_like_gibberish(answer):\n",
    "                    docs = (res.get('source_documents') if isinstance(res, dict) else []) or RETRIEVER.get_relevant_documents(question)\n",
    "                    snippets = [d.page_content.strip()[:500] for d in docs[:3] if d.page_content]\n",
    "                    answer = 'I could not form a clear answer. Relevant guideline snippets:\\n\\n' + (\"\\n\\n---\\n\\n\".join(snippets) if snippets else 'No guideline text found.')\n",
    "            except Exception as e:\n",
    "                answer = f'RAG error: {e}'\n",
    "\n",
    "        return xray_res, summary, answer\n",
    "\n",
    "    # bind the button inside the Blocks\n",
    "    btn.click(run_pipeline, inputs=[xray, report, question], outputs=[xray_out, summary_out, answer_out])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
