{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple, List, Optional\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------- image transforms ----------\n",
    "XRAY_TFMS = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------- small helper: find inner torch module from wrappers ----------\n",
    "def _extract_torch_module(m):\n",
    "    if isinstance(m, nn.Module):\n",
    "        return m\n",
    "    # common wrapper attribute names\n",
    "    for name in [\"model\", \"network\", \"net\", \"module\", \"backbone\", \"base_model\"]:\n",
    "        if hasattr(m, name) and isinstance(getattr(m, name), nn.Module):\n",
    "            return getattr(m, name)\n",
    "    raise AttributeError(\"Could not find inner nn.Module inside the provided model/wrapper.\")\n",
    "\n",
    "# ---------- call this in your TRAINING notebook after training ----------\n",
    "def export_xray_checkpoint(\n",
    "    trained_model,\n",
    "    class_names: List[str],\n",
    "    export_path: str,\n",
    "    arch_hint: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves a universal checkpoint:\n",
    "      { 'state_dict': ..., 'classes': [...], 'arch': 'resnet18', 'num_classes': N }\n",
    "    Works whether trained_model is a wrapper or a plain nn.Module.\n",
    "    \"\"\"\n",
    "    torch_model = _extract_torch_module(trained_model)\n",
    "    # infer arch if not provided\n",
    "    arch = arch_hint or torch_model.__class__.__name__.lower()\n",
    "    # some wrappers keep the head as .fc (resnet) or .classifier (densenet/efficientnet)\n",
    "    if hasattr(torch_model, \"fc\") and isinstance(torch_model.fc, nn.Module):\n",
    "        out_features = getattr(torch_model.fc, \"out_features\", len(class_names))\n",
    "    elif hasattr(torch_model, \"classifier\") and isinstance(torch_model.classifier, nn.Module):\n",
    "        out_features = getattr(torch_model.classifier, \"out_features\", len(class_names))\n",
    "    else:\n",
    "        out_features = len(class_names)\n",
    "\n",
    "    payload = {\n",
    "        \"state_dict\": torch_model.state_dict(),\n",
    "        \"classes\": list(class_names),\n",
    "        \"arch\": arch,                 # e.g., 'resnet18', 'resnet34', 'densenet121'\n",
    "        \"num_classes\": int(out_features),\n",
    "        \"input_size\": (224, 224),\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(export_path), exist_ok=True)\n",
    "    torch.save(payload, export_path)\n",
    "    return export_path\n",
    "\n",
    "# ---------- mapping from arch name -> constructor + head patch ----------\n",
    "def _build_backbone(arch: str, num_classes: int) -> nn.Module:\n",
    "    arch = (arch or \"resnet18\").lower()\n",
    "\n",
    "    if arch in [\"resnet18\", \"resnet-18\", \"resnet\"]:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "        return m\n",
    "    if arch in [\"resnet34\", \"resnet-34\"]:\n",
    "        m = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "        return m\n",
    "    if arch in [\"resnet50\", \"resnet-50\"]:\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.fc.in_features\n",
    "        m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "        return m\n",
    "    if arch in [\"densenet121\", \"densenet-121\", \"densenet\"]:\n",
    "        m = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.classifier.in_features\n",
    "        m.classifier = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "        return m\n",
    "    # fallback\n",
    "    m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(in_feats, num_classes))\n",
    "    return m\n",
    "\n",
    "# ---------- loader used by main ----------\n",
    "def load_xray_model(checkpoint_path: str) -> Tuple[nn.Module, List[str]]:\n",
    "    \"\"\"\n",
    "    Loads from:\n",
    "      - universal dict: {'state_dict', 'classes', 'arch', 'num_classes'}\n",
    "      - OR plain state_dict (weights-only), defaulting to resnet18 + 2 classes\n",
    "    \"\"\"\n",
    "    ckpt = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        classes = ckpt.get(\"classes\", [\"NORMAL\", \"PNEUMONIA\"])\n",
    "        num_classes = ckpt.get(\"num_classes\", len(classes))\n",
    "        arch = ckpt.get(\"arch\", \"resnet18\")\n",
    "        model = _build_backbone(arch, num_classes)\n",
    "        model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "    else:\n",
    "        # weights-only: assume resnet18 with 2 classes\n",
    "        classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "        model = _build_backbone(\"resnet18\", len(classes))\n",
    "        model.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "    model.eval().to(DEVICE)\n",
    "    return model, classes\n",
    "\n",
    "# ---------- prediction ----------\n",
    "def predict_xray(model: nn.Module, img) -> Tuple[int, float, list]:\n",
    "    \"\"\"\n",
    "    Returns (pred_idx, conf, probs_list) with test-time flip averaging.\n",
    "    \"\"\"\n",
    "    if isinstance(img, (str, os.PathLike)):\n",
    "        img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "    x1 = XRAY_TFMS(img).unsqueeze(0).to(DEVICE)\n",
    "    x2 = XRAY_TFMS(img.transpose(Image.FLIP_LEFT_RIGHT)).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p1 = torch.softmax(model(x1), dim=1)\n",
    "        p2 = torch.softmax(model(x2), dim=1)\n",
    "        probs = ((p1 + p2) / 2.0).squeeze(0).cpu().numpy()\n",
    "\n",
    "    idx = int(probs.argmax())\n",
    "    return idx, float(probs[idx]), probs.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eeb59a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_xray_checkpoint\n\u001b[32m      2\u001b[39m export_xray_checkpoint(trained_model=model,               \u001b[38;5;66;03m# your wrapper or nn.Module\u001b[39;00m\n\u001b[32m      3\u001b[39m                        class_names=[\u001b[33m\"\u001b[39m\u001b[33mNORMAL\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mPNEUMONIA\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m                        export_path=os.path.join(ART_DIR,\u001b[33m\"\u001b[39m\u001b[33mxray_model.pth\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m                        arch_hint=\u001b[33m\"\u001b[39m\u001b[33mresnet18\u001b[39m\u001b[33m\"\u001b[39m)             \u001b[38;5;66;03m# change if you used another backbone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils import export_xray_checkpoint\n",
    "export_xray_checkpoint(trained_model=model,               # your wrapper or nn.Module\n",
    "                       class_names=[\"NORMAL\",\"PNEUMONIA\"],\n",
    "                       export_path=os.path.join(ART_DIR,\"xray_model.pth\"),\n",
    "                       arch_hint=\"resnet18\")             # change if you used another backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, torch\n",
    "# from torch import nn\n",
    "# from torchvision import models, transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # Absolute paths to artifacts so app runs from anywhere\n",
    "# ROOT = r\"D:\\\\MedicalAI-Assistant\"\n",
    "# ART_DIR = os.path.join(ROOT, 'artifacts')\n",
    "\n",
    "# def load_xray_model(weights_path: str, class_names=('NORMAL','PNEUMONIA')):\n",
    "#     model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "#     num_feats = model.fc.in_features\n",
    "#     model.fc = nn.Sequential(\n",
    "#         nn.Dropout(0.2),\n",
    "#         nn.Linear(num_feats, len(class_names))\n",
    "#     )\n",
    "#     sd = torch.load(weights_path, map_location=DEVICE)\n",
    "#     model.load_state_dict(sd['state_dict'])\n",
    "#     model.eval().to(DEVICE)\n",
    "#     return model, sd.get('classes', list(class_names))\n",
    "\n",
    "# _tfms = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# #def predict_xray(model, img):\n",
    "#     #if isinstance(img, (str, os.PathLike)):\n",
    "#         #img = Image.open(img).convert('RGB')\n",
    "#     #x = _tfms(img).unsqueeze(0).to(DEVICE)\n",
    "#     #with torch.no_grad():\n",
    "#         #logits = model(x)\n",
    "#         ##probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "#         #pred_idx = probs.argmax()\n",
    "#     #return pred_idx, float(probs[pred_idx]), probs.tolist()\n",
    "# def predict_xray_with_roboflow(img_path):\n",
    "#     pred, conf, _ = roboflow_classify(img_path)\n",
    "#     return pred, conf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
